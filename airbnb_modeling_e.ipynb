{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook does not contain all the models, as they were changed in place during the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# general\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# modelling\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler \n",
    "from sklearn.model_selection import train_test_split, cross_val_score \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "from sklearn.metrics import explained_variance_score, mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Set plot preference\n",
    "plt.style.use(style='ggplot')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from IPython.display import SVG\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "from keras import models, layers, optimizers, regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb = pd.read_csv(\"airbnb4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = [\"Unnamed: 0\", \"Unnamed: 0.1\",\"longitude\", \"latitude\",\"id\", \"air_nodes_true\", \"coordinates\"]\n",
    "\n",
    "airbnb = airbnb.drop(to_drop, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numerical_columns = ['accommodates', 'price', 'days_being_host', 'maximum_nights', 'minimum_nights',\n",
    "                     \"availability_365\", 'number_of_reviews', \"calculated_host_listings_count\",\n",
    "                     \"cleaning_fee\", \"security_deposit\", \"bathrooms\", \"extra_people\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transforming columns (numerical_raw)\n",
    "numerical_columns = [i for i in numerical_columns if i not in [\"days_being_host\", 'availability_365']] # Removing items not to be transformed\n",
    "\n",
    "for col in numerical_columns:\n",
    "    airbnb[col] = airbnb[col].astype('float64').replace(0.0, 0.01) # Replacing 0s with 0.01\n",
    "    airbnb[col] = np.log(airbnb[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop_walk = [\"geometry_distance_to_park\", \"geometry_distance_to_supermarket\", \"geometry_distance_to_restaurant\",\n",
    "                \"geometry_distance_to_station\"]\n",
    "\n",
    "airbnb_walk = airbnb.drop(to_drop_walk, axis = 1)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop_walk = [\"geometry_distance_to_park\", \"geometry_distance_to_supermarket\", \"geometry_distance_to_restaurant\"]\n",
    "airbnb_walk = airbnb.drop(to_drop_walk, axis = 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate the dataframe, where we will keep the results (up to NN)\n",
    "results = pd.DataFrame() \n",
    "columns = [\"Algorithm\",\"TrainMSE\", \"MSE\",\"TrainR2\", \"R2\", \"Parameters\"]\n",
    "\n",
    "results = pd.DataFrame(columns = columns)\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating X and y\n",
    "X = airbnb_walk.drop('price', axis=1)\n",
    "y = airbnb_walk.price\n",
    "\n",
    "# Splitting into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2)\n",
    "\n",
    "lin_reg = LinearRegression()  \n",
    "lin_reg.fit(X_train, y_train) # training the algorithm\n",
    "\n",
    "# Predict\n",
    "train_pred_lin_reg = lin_reg.predict(X_train)\n",
    "pred_lin_reg = lin_reg.predict(X_test)\n",
    "\n",
    "# Check the predictions against the actual values by using the MSE and R-2 metrics:\n",
    "print(\"Validation MSE:\", round(mean_squared_error(y_test, pred_lin_reg),4))\n",
    "print(\"Validation r2:\", round(r2_score(y_test, pred_lin_reg),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc[i] = [\"Linear Regression\",\n",
    "                  mean_squared_error(y_train, lin_reg.predict(X_train)),\n",
    "                  mean_squared_error(y_test, pred_lin_reg),\n",
    "                  r2_score(y_train, lin_reg.predict(X_train)),\n",
    "                  r2_score(y_test, pred_lin_reg),\n",
    "                  \"-\"]\n",
    "\n",
    "i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Separating X and y\n",
    "X = airbnb_walk.drop('price', axis=1)\n",
    "y = airbnb_walk.price\n",
    "\n",
    "# Splitting into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2)\n",
    "\n",
    "xgb_reg = xgb.XGBRegressor()\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "\n",
    "train_pred_xgb = xgb_reg.predict(X_train)\n",
    "val_pred_xgb = xgb_reg.predict(X_test)\n",
    "\n",
    "print(\"\\nTraining MSE:\", round(mean_squared_error(y_train, train_pred_xgb),4))\n",
    "print(\"Validation MSE:\", round(mean_squared_error(y_test, val_pred_xgb),4))\n",
    "\n",
    "print(\"\\nTraining r2:\", round(r2_score(y_train, train_pred_xgb),4))\n",
    "print(\"Validation r2:\", round(r2_score(y_test, val_pred_xgb),4)) # Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc[i] = [\"Xgboost1\",\n",
    "                  mean_squared_error(y_train, train_pred_xgb),\n",
    "                  mean_squared_error(y_test, val_pred_xgb),\n",
    "                  r2_score(y_train, train_pred_xgb),\n",
    "                  r2_score(y_test, val_pred_xgb),\n",
    "                  xgb_reg.get_params()]\n",
    "\n",
    "i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "params = {\n",
    "    # Parameters that we are going to tune.\n",
    "    \"max_depth\" : [8,9,10],\n",
    "    \"min_child_weight\": [4,5,6,7],\n",
    "    'eta':[0.005, .05, .01, 0.1],\n",
    "    'subsample': [i/10. for i in range(7,11)],\n",
    "    'colsample_bytree': [i/10. for i in range(7,11)],\n",
    "    # Other parameters\n",
    "    \"n_estimators\" : [100,120, 150] }\n",
    "\n",
    "scoring = {'R2': 'r2', 'MSE': \"neg_mean_squared_error\"}\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "\n",
    "CV_xgb = GridSearchCV(estimator=xgb_model, param_grid=params,scoring= scoring,refit = \"R2\", return_train_score=True, cv = 3, n_jobs = -1)\n",
    "CV_xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_reg = CV_xgb.best_estimator_\n",
    "\n",
    "val_pred_xgb = xg_reg.predict(X_test)\n",
    "\n",
    "print(\"Training MSE:\", round(mean_squared_error(y_train, xg_reg.predict(X_train)),4))\n",
    "print(\"Validation MSE:\", round(mean_squared_error(y_test, val_pred_xgb),4))\n",
    "\n",
    "print(\"Training r2:\", round(r2_score(y_train,  xg_reg.predict(X_train)),4))\n",
    "print(\"Validation r2:\", round(r2_score(y_test, val_pred_xgb),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc[i] = [\"Xgboost2\",\n",
    "                  mean_squared_error(y_train, xg_reg.predict(X_train)),\n",
    "                  mean_squared_error(y_test, val_pred_xgb),\n",
    "                  r2_score(y_train,  xg_reg.predict(X_train)),\n",
    "                  r2_score(y_test, val_pred_xgb),\n",
    "                  xgb_reg.get_params()]\n",
    "\n",
    "i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_weights_xgb_reg = pd.DataFrame(xg_reg.feature_importances_, columns=['weight'], index=X_train.columns)\n",
    "ft_weights_xgb_reg.sort_values('weight', inplace=True)\n",
    "ft_weights_xgb_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting feature importances\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.barh(ft_weights_xgb_reg.index, ft_weights_xgb_reg.weight, align='center') \n",
    "plt.title(\"Feature importances in the XGBoost model\", fontsize=14)\n",
    "plt.xlabel(\"Feature importance\")\n",
    "plt.margins(y=0.01)\n",
    "plt.savefig(\"Feature Importance XGBoost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating X and y\n",
    "X = airbnb_walk.drop('price', axis=1)\n",
    "y = airbnb_walk.price\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2)\n",
    "\n",
    "# Scale \n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "train_pred_rf = rf.predict(X_train)\n",
    "val_pred_rf = rf.predict(X_test)\n",
    "\n",
    "print(\"Validation MSE:\", round(mean_squared_error(y_test, val_pred_rf),4))\n",
    "print(\"Validation r2:\", round(r2_score(y_test, val_pred_rf),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc[i] = [\"RFR1\",\n",
    "                  mean_squared_error(y_train, train_pred_rf),\n",
    "                  mean_squared_error(y_test, val_pred_rf),\n",
    "                  r2_score(y_train, train_pred_rf),\n",
    "                  r2_score(y_test, val_pred_rf),\n",
    "                  rf.get_params()]\n",
    "\n",
    "i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "scoring = {'R2': 'r2', 'MSE': \"neg_mean_squared_error\"}\n",
    "\n",
    "param_grid = { \n",
    "    'n_estimators': [100,120,150,200],\n",
    "    'max_features': [10, 20, 30],\n",
    "    'max_depth' : [8,9,10],\n",
    "    \"min_samples_leaf\" :[3,4],\n",
    "    \"min_samples_split\":[8,9]\n",
    "}\n",
    "\n",
    "CV_rfc_tune = GridSearchCV(estimator=rf, param_grid=param_grid, cv = 3, scoring= scoring,refit = \"R2\", return_train_score=True, n_jobs =-1)\n",
    "CV_rfc_tune.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best = CV_rfc_tune.best_estimator_\n",
    "val_pred_rf = rf_best.predict(X_test)\n",
    "\n",
    "print(\"Validation MSE:\", round(mean_squared_error(y_test, val_pred_rf),4))\n",
    "print(\"Validation r2:\", round(r2_score(y_test, val_pred_rf),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc[i] = [\"RFR2\",\n",
    "                  mean_squared_error(y_train, rf_best.predict(X_train)),\n",
    "                  mean_squared_error(y_test, val_pred_rf),\n",
    "                  r2_score(y_train, rf_best.predict(X_train)),\n",
    "                  r2_score(y_test, val_pred_rf),\n",
    "                  rf_best.get_params()]\n",
    "\n",
    "i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2)\n",
    "\n",
    "# Scale \n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "svr = SVR(kernel = \"rbf\", C = 1, gamma = 0.005, epsilon = 0.05)\n",
    "svr.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "train_pred_svr = svr.predict(X_train)\n",
    "val_pred_svr = svr.predict(X_test)\n",
    "\n",
    "print(\"\\nTraining MSE:\", round(mean_squared_error(y_train, train_pred_svr),4))\n",
    "print(\"Validation MSE:\", round(mean_squared_error(y_test, val_pred_svr),4))\n",
    "\n",
    "print(\"\\nTraining r2:\", round(r2_score(y_train, train_pred_svr),4))\n",
    "print(\"Validation r2:\", round(r2_score(y_test, val_pred_svr),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc[i] = [\"SVR1\",\n",
    "                  mean_squared_error(y_train, train_pred_svr),\n",
    "                  mean_squared_error(y_test, val_pred_svr),\n",
    "                  r2_score(y_train, train_pred_svr),\n",
    "                  r2_score(y_test, val_pred_svr),\n",
    "                  svr.get_params()]\n",
    "\n",
    "i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "svr=SVR()\n",
    "\n",
    "grid_params_svr =  {'C': [1, 10], \n",
    "                    'kernel': [ \"rbf\", \"linear\"],\n",
    "                   \"gamma\": [0.0001,0.001, 0.01, 0.1],\n",
    "                   \"epsilon\": [0.01, 0.05, 0.1, 0.5]}\n",
    "scoring = {'R2': 'r2', 'MSE': \"neg_mean_squared_error\"}\n",
    "\n",
    "CV_svr = GridSearchCV(estimator=svr, param_grid=grid_params_svr, cv = 3, n_jobs = -1, verbose = 0, scoring= scoring, refit = \"R2\",return_train_score=True)\n",
    "CV_svr.fit(X_train, y_train)\n",
    "\n",
    "print(CV_svr.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr = CV_svr.best_estimator_\n",
    "svr.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "val_pred_svr = svr.predict(X_test)\n",
    "\n",
    "print(\"Validation MSE:\", round(mean_squared_error(y_test, val_pred_svr),4))\n",
    "print(\"Validation r2:\", round(r2_score(y_test, val_pred_svr),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.loc[i] = [\"SVR2\",\n",
    "                  mean_squared_error(y_train, svr.predict(X_train)),\n",
    "                  mean_squared_error(y_test, val_pred_svr),\n",
    "                  r2_score(y_train, svr.predict(X_train)),\n",
    "                  r2_score(y_test, val_pred_svr),\n",
    "                  CV_svr.get_params()]\n",
    "\n",
    "i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"grid search\"\n",
    "#lr = [ 10 ** x for x in range(-5,1)]\n",
    "#settings = []\n",
    "#for act in [\"relu\", \"tanh\"]:\n",
    "#    for eta0 in lr:\n",
    "#        for bs in [64, 128, 256]:\n",
    "#            for e in [100,150,200]:\n",
    "#                model = models.Sequential()\n",
    "#                model.add(layers.Dense(128, input_shape=(X_train.shape[1],), activation = act))\n",
    "#                model.add(layers.Dense(256, activation = act))\n",
    "#                model.add(layers.Dense(256, activation = act))\n",
    "#                model.add(layers.Dense(256, activation = act))\n",
    "#                model.add(layers.Dense(1, activation = \"linear\"))\n",
    "#                opt =  optimizers.Adam(learning_rate=eta0)\n",
    "#                model.compile(optimizer = opt, loss = \"mean_squared_error\", metrics = [\"mean_squared_error\"])\n",
    "#                model.fit(X_train, y_train, batch_size=bs, epochs=e, verbose=1)\n",
    "#                y_pred = model.predict(X_test, verbose=0)\n",
    "#                r2 = r2_score(y_test, y_pred)\n",
    "#                mse = mean_squared_error(y_test,y_pred)\n",
    "#                settings.append((dns, act, eta0, bs, e, mse,r2))\n",
    "#                print(settings[-1])\n",
    "\n",
    "#best_r2 =  max(settings, key=lambda x: x[-1])\n",
    "#print(\"Best settings according to R2 {}\".format(best_r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating X and y\n",
    "X = airbnb_walk.drop('price', axis=1)\n",
    "y = airbnb_walk.price\n",
    "\n",
    "# Splitting into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state=123)\n",
    "\n",
    "# Scale \n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn1_relu = models.Sequential()\n",
    "nn1_relu.add(layers.Dense(64, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "nn1_relu.add(layers.Dense(64, activation='relu'))\n",
    "nn1_relu.add(layers.Dense(32, activation='relu'))\n",
    "nn1_relu.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "# opt =  optimizers.Adam(learning_rate=0.001)\n",
    "nn1_relu.compile(optimizer = \"sgd\", loss = \"mean_squared_error\", metrics = [\"mean_squared_error\"])\n",
    "\n",
    "# fit the model\n",
    "nn1_relu1_history = nn1_relu.fit(X_train,\n",
    "                      y_train,\n",
    "                      epochs=300,\n",
    "                      batch_size=64,\n",
    "                      validation_split = 0.2,\n",
    "                      verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history1 = pd.DataFrame(nn1_relu1_history)\n",
    "plt.figure(figsize=(6,6));\n",
    "loss_plt = plt.plot(history1[\"val_loss\"], label = \"Loss\");\n",
    "accuracy_plt = plt.plot(history1[\"mean_squared_error\"], label = \"MSE\");\n",
    "plt.legend(bbox_to_anchor=(1, 1))\n",
    "plt.title(\"Loss vs MSE\");\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model\n",
    "nn1_relu2 = models.Sequential()\n",
    "nn1_relu2.add(layers.Dense(128, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "nn1_relu2.add(layers.Dense(256, activation='relu'))\n",
    "nn1_relu2.add(layers.Dense(256, activation='relu'))\n",
    "nn1_relu2.add(layers.Dense(256, activation='relu'))\n",
    "nn1_relu2.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "# Compiling the model\n",
    "nn1_relu2.compile(loss='mean_squared_error',\n",
    "            optimizer='sgd',\n",
    "            metrics=['mean_squared_error'])\n",
    "\n",
    "# Training the model\n",
    "nn1_relu2_history = nn1_relu2.fit(X_train,\n",
    "                  y_train,\n",
    "                  epochs=300,\n",
    "                  batch_size=128,\n",
    "                  validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = pd.DataFrame(nn1_relu2_history)\n",
    "plt.figure(figsize=(6,6));\n",
    "loss_plt = plt.plot(history2[\"val_loss\"], label = \"Loss\");\n",
    "accuracy_plt = plt.plot(history2[\"mean_squared_error\"], label = \"MSE\");\n",
    "plt.legend(bbox_to_anchor=(1, 1))\n",
    "plt.title(\"Loss vs MSE\");\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model\n",
    "nn1_relu3 = models.Sequential()\n",
    "nn1_relu3.add(layers.Dense(128, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "nn1_relu3.add(layers.Dense(256, activation='relu'))\n",
    "nn1_relu3.add(layers.Dense(256, activation='relu'))\n",
    "nn1_relu3.add(layers.Dense(256, activation='relu'))\n",
    "nn1_relu3.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "# Compiling the model\n",
    "nn1_relu3.compile(loss='mean_squared_error',\n",
    "            optimizer='adam',\n",
    "            metrics=['mean_squared_error'])\n",
    "\n",
    "# Training the model\n",
    "nn1_relu3_history = nn1_relu3.fit(X_train,\n",
    "                  y_train,\n",
    "                  epochs=300,\n",
    "                  batch_size=128,\n",
    "                  validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model\n",
    "nn1_relu4 = models.Sequential()\n",
    "nn1_relu4.add(layers.Dense(128, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "nn1_relu4.add(layers.Dense(256, activation='relu'))\n",
    "nn1_relu4.add(layers.Dense(256, activation='relu'))\n",
    "nn1_relu4.add(layers.Dense(512, activation='relu'))\n",
    "nn1_relu4.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "# Compiling the model\n",
    "nn1_relu4.compile(loss='mean_squared_error',\n",
    "            optimizer='adam',\n",
    "            metrics=['mean_squared_error']) \n",
    "\n",
    "# Training the model\n",
    "nn1_relu4_history = nn1_relu4.fit(X_train,\n",
    "                  y_train,\n",
    "                  epochs=200,\n",
    "                  batch_size=128,\n",
    "                  validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L1 regularization: 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn1_relu4_r = models.Sequential()\n",
    "nn1_relu4_r.add(layers.Dense(128, input_shape=(X_train.shape[1],),kernel_regularizer=regularizers.l1(0.0001), activation='relu'))\n",
    "nn1_relu4_r.add(layers.Dense(256,kernel_regularizer=regularizers.l1(0.0001), activation='relu'))\n",
    "nn1_relu4_r.add(layers.Dense(256,kernel_regularizer=regularizers.l1(0.0001), activation='relu'))\n",
    "nn1_relu4_r.add(layers.Dense(512,kernel_regularizer=regularizers.l1(0.0001), activation='relu'))\n",
    "nn1_relu4_r.add(layers.Dense(1,kernel_regularizer=regularizers.l1(0.0001), activation='linear'))\n",
    "\n",
    "# Compiling the model\n",
    "nn1_relu4_r.compile(loss='mean_squared_error',\n",
    "            optimizer='adam',\n",
    "            metrics=['mean_squared_error'])\n",
    "\n",
    "# Training the model\n",
    "nn1_relu4_r_history = nn1_relu4_r.fit(X_train,\n",
    "                  y_train,\n",
    "                  epochs=200,\n",
    "                  batch_size=128,\n",
    "                  validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L1 regularization: 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn1_relu4_r2 = models.Sequential()\n",
    "nn1_relu4_r2.add(layers.Dense(128, input_shape=(X_train.shape[1],),kernel_regularizer=regularizers.l1(0.0005), activation='relu'))\n",
    "nn1_relu4_r2.add(layers.Dense(256,kernel_regularizer=regularizers.l1(0.0005), activation='relu'))\n",
    "nn1_relu4_r2.add(layers.Dense(256,kernel_regularizer=regularizers.l1(0.0005), activation='relu'))\n",
    "nn1_relu4_r2.add(layers.Dense(512,kernel_regularizer=regularizers.l1(0.0005), activation='relu'))\n",
    "nn1_relu4_r2.add(layers.Dense(1,kernel_regularizer=regularizers.l1(0.0005), activation='linear'))\n",
    "\n",
    "# Compiling the model\n",
    "nn1_relu4_r2.compile(loss='mean_squared_error',\n",
    "            optimizer='adam',\n",
    "            metrics=['mean_squared_error'])\n",
    "\n",
    "# Training the model\n",
    "nn1_relu4_r2_history = nn1_relu4_r2.fit(X_train,\n",
    "                  y_train,\n",
    "                  epochs=200,\n",
    "                  batch_size=128,\n",
    "                  validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L1 reg. 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn1_relu4_r3 = models.Sequential()\n",
    "nn1_relu4_r3.add(layers.Dense(128, input_shape=(X_train.shape[1],),kernel_regularizer=regularizers.l1(0.001), activation='relu'))\n",
    "nn1_relu4_r3.add(layers.Dense(256,kernel_regularizer=regularizers.l1(0.001), activation='relu'))\n",
    "nn1_relu4_r3.add(layers.Dense(256,kernel_regularizer=regularizers.l1(0.001), activation='relu'))\n",
    "nn1_relu4_r3.add(layers.Dense(512,kernel_regularizer=regularizers.l1(0.001), activation='relu'))\n",
    "nn1_relu4_r3.add(layers.Dense(1,kernel_regularizer=regularizers.l1(0.001), activation='linear'))\n",
    "\n",
    "# Compiling the model\n",
    "nn1_relu4_r3.compile(loss='mean_squared_error',\n",
    "            optimizer='adam',\n",
    "            metrics=['mean_squared_error'])\n",
    "# Printing the model summary\n",
    "print(nn1_relu4_r3.summary())\n",
    "\n",
    "# Visualising the neural network\n",
    "#SVG(model_to_dot(nn2, show_layer_names=False, show_shapes=True).create(prog='dot', format='svg'))\n",
    "\n",
    "# Training the model\n",
    "nn1_relu4_r3_history = nn1_relu4_r3.fit(X_train,\n",
    "                  y_train,\n",
    "                  epochs=200,\n",
    "                  batch_size=128,\n",
    "                  validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L1 reg. 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn1_relu4_r4 = models.Sequential()\n",
    "nn1_relu4_r4.add(layers.Dense(128, input_shape=(X_train.shape[1],),kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "nn1_relu4_r4.add(layers.Dense(256,kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "nn1_relu4_r4.add(layers.Dense(256,kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "nn1_relu4_r4.add(layers.Dense(512,kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "nn1_relu4_r4.add(layers.Dense(1,kernel_regularizer=regularizers.l1(0.005), activation='linear'))\n",
    "\n",
    "# Compiling the model\n",
    "nn1_relu4_r4.compile(loss='mean_squared_error',\n",
    "            optimizer='adam',\n",
    "            metrics=['mean_squared_error'])\n",
    "# Printing the model summary\n",
    "print(nn1_relu4_r4.summary())\n",
    "\n",
    "# Visualising the neural network\n",
    "#SVG(model_to_dot(nn2, show_layer_names=False, show_shapes=True).create(prog='dot', format='svg'))\n",
    "\n",
    "# Training the model\n",
    "nn1_relu4_r4_history = nn1_relu4_r4.fit(X_train,\n",
    "                  y_train,\n",
    "                  epochs=200,\n",
    "                  batch_size=128,\n",
    "                  validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L1 reg. 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn1_relu4_r5 = models.Sequential()\n",
    "nn1_relu4_r5.add(layers.Dense(128, input_shape=(X_train.shape[1],),kernel_regularizer=regularizers.l1(0.01), activation='relu'))\n",
    "nn1_relu4_r5.add(layers.Dense(256,kernel_regularizer=regularizers.l1(0.01), activation='relu'))\n",
    "nn1_relu4_r5.add(layers.Dense(256,kernel_regularizer=regularizers.l1(0.01), activation='relu'))\n",
    "nn1_relu4_r5.add(layers.Dense(512,kernel_regularizer=regularizers.l1(0.01), activation='relu'))\n",
    "nn1_relu4_r5.add(layers.Dense(1,kernel_regularizer=regularizers.l1(0.01), activation='linear'))\n",
    "\n",
    "# Compiling the model\n",
    "nn1_relu4_r5.compile(loss='mean_squared_error',\n",
    "            optimizer='adam',\n",
    "            metrics=['mean_squared_error'])\n",
    "# Printing the model summary\n",
    "print(nn1_relu4_r5.summary())\n",
    "\n",
    "# Visualising the neural network\n",
    "#SVG(model_to_dot(nn2, show_layer_names=False, show_shapes=True).create(prog='dot', format='svg'))\n",
    "\n",
    "# Training the model\n",
    "nn1_relu4_r5_history = nn1_relu4_r5.fit(X_train,\n",
    "                  y_train,\n",
    "                  epochs=200,\n",
    "                  batch_size=128,\n",
    "                  validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L2 regularization:  0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn1_relu4_r_l2_1 = models.Sequential()\n",
    "nn1_relu4_r_l2_1.add(layers.Dense(128, input_shape=(X_train.shape[1],),kernel_regularizer=regularizers.l2(0.0001), activation='relu'))\n",
    "nn1_relu4_r_l2_1.add(layers.Dense(256,kernel_regularizer=regularizers.l2(0.0001), activation='relu'))\n",
    "nn1_relu4_r_l2_1.add(layers.Dense(256,kernel_regularizer=regularizers.l2(0.0001), activation='relu'))\n",
    "nn1_relu4_r_l2_1.add(layers.Dense(512,kernel_regularizer=regularizers.l2(0.0001), activation='relu'))\n",
    "nn1_relu4_r_l2_1.add(layers.Dense(1,kernel_regularizer=regularizers.l2(0.0001), activation='linear'))\n",
    "\n",
    "# Compiling the model\n",
    "nn1_relu4_r_l2_1.compile(loss='mean_squared_error',\n",
    "            optimizer='adam',\n",
    "            metrics=['mean_squared_error'])\n",
    "\n",
    "# Training the model\n",
    "nn1_relu4_r_l2_1_history = nn1_relu4_r_l2_1.fit(X_train,\n",
    "                  y_train,\n",
    "                  epochs=150,\n",
    "                  batch_size=256,\n",
    "                  validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L2 regularization:  0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn1_relu4_r_l2_2 = models.Sequential()\n",
    "nn1_relu4_r_l2_2.add(layers.Dense(128, input_shape=(X_train.shape[1],),kernel_regularizer=regularizers.l2(0.001), activation='relu'))\n",
    "nn1_relu4_r_l2_2.add(layers.Dense(256,kernel_regularizer=regularizers.l2(0.001), activation='relu'))\n",
    "nn1_relu4_r_l2_2.add(layers.Dense(256,kernel_regularizer=regularizers.l2(0.001), activation='relu'))\n",
    "nn1_relu4_r_l2_2.add(layers.Dense(512,kernel_regularizer=regularizers.l2(0.001), activation='relu'))\n",
    "nn1_relu4_r_l2_2.add(layers.Dense(1,kernel_regularizer=regularizers.l2(0.001), activation='linear'))\n",
    "\n",
    "# Compiling the model\n",
    "nn1_relu4_r_l2_2.compile(loss='mean_squared_error',\n",
    "            optimizer='adam',\n",
    "            metrics=['mean_squared_error'])\n",
    "# Printing the model summary\n",
    "print(nn1_relu4_r_l2_2.summary())\n",
    "\n",
    "# Visualising the neural network\n",
    "#SVG(model_to_dot(nn2, show_layer_names=False, show_shapes=True).create(prog='dot', format='svg'))\n",
    "\n",
    "# Training the model\n",
    "nn1_relu4_r_l2_2_history = nn1_relu4_r_l2_2.fit(X_train,\n",
    "                  y_train,\n",
    "                  epochs=150,\n",
    "                  batch_size=256,\n",
    "                  validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L2 regularization:  0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn1_relu4_r_l2_3 = models.Sequential()\n",
    "nn1_relu4_r_l2_3.add(layers.Dense(128, input_shape=(X_train.shape[1],),kernel_regularizer=regularizers.l2(0.01), activation='relu'))\n",
    "nn1_relu4_r_l2_3.add(layers.Dense(256,kernel_regularizer=regularizers.l2(0.01), activation='relu'))\n",
    "nn1_relu4_r_l2_3.add(layers.Dense(256,kernel_regularizer=regularizers.l2(0.01), activation='relu'))\n",
    "nn1_relu4_r_l2_3.add(layers.Dense(512,kernel_regularizer=regularizers.l2(0.01), activation='relu'))\n",
    "nn1_relu4_r_l2_3.add(layers.Dense(1,kernel_regularizer=regularizers.l2(0.01), activation='linear'))\n",
    "\n",
    "# Compiling the model\n",
    "nn1_relu4_r_l2_3.compile(optimizer = \"adam\", loss = \"mean_squared_error\", metrics = [\"mean_squared_error\"])\n",
    "\n",
    "# Training the model\n",
    "nn1_relu4_r_l2_3_history = nn1_relu4_r_l2_3.fit(X_train,\n",
    "                  y_train,\n",
    "                  epochs=150,\n",
    "                  batch_size=256,\n",
    "                  validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Tanh activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best setting with tanh activation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn1_tanh_r = models.Sequential()\n",
    "nn1_tanh_r.add(layers.Dense(128, input_shape=(X_train.shape[1],),kernel_regularizer=regularizers.l2(0.007), activation='tanh'))\n",
    "nn1_tanh_r.add(layers.Dense(256,kernel_regularizer=regularizers.l2(0.007), activation='tanh'))\n",
    "nn1_tanh_r.add(layers.Dense(256,kernel_regularizer=regularizers.l2(0.007), activation='tanh'))\n",
    "nn1_tanh_r.add(layers.Dense(256,kernel_regularizer=regularizers.l2(0.007), activation='tanh'))\n",
    "nn1_tanh_r.add(layers.Dense(512,kernel_regularizer=regularizers.l2(0.007), activation='tanh'))\n",
    "nn1_tanh_r.add(layers.Dense(1,kernel_regularizer=regularizers.l2(0.007), activation='linear'))\n",
    "\n",
    "# Compiling the model\n",
    "opt =  optimizers.Adam(learning_rate=0.01)\n",
    "nn1_tanh_r.compile(optimizer = opt, loss = \"mean_squared_error\", metrics = [\"mean_squared_error\"])\n",
    "# Printing the model summary\n",
    "\n",
    "# Training the model\n",
    "nn1_tanh_r_history = nn1_tanh_r.fit(X_train,\n",
    "                  y_train,\n",
    "                  epochs=150,\n",
    "                  batch_size=256,\n",
    "                  validation_split = 0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
