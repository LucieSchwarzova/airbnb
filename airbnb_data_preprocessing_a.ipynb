{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pre-processing part is mainly based on the notebook by Laura Lewis\n",
    "https://nbviewer.jupyter.org/github/L-Lewis/Airbnb-neural-network-price-prediction/blob/master/Airbnb-price-prediction.ipynb#Categorical-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_original = pd.read_csv(\"listings_june_2019_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_original.ndim)\n",
    "print(data_original.shape)\n",
    "# 106 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', len(data_original.columns)) \n",
    "pd.set_option('display.max_rows', 106)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all columns\n",
    "data_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On the first sight, we might delete columns, that: \n",
    "# 1) contains the same information throughout the dataset\n",
    "# 2) contains text data - this is out f the scope of this analysis, therefore will be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['listing_url', 'scrape_id', 'last_scraped', 'name', 'summary', 'space', 'description', 'neighborhood_overview',\n",
    "                'notes', 'transit', 'access', 'interaction', 'house_rules', 'thumbnail_url', 'medium_url', 'picture_url',\n",
    "                'xl_picture_url', 'host_id', 'host_url', 'host_name', 'host_about', 'host_thumbnail_url',\n",
    "                'host_picture_url', 'host_verifications', 'calendar_last_scraped']\n",
    "\n",
    "data = data_original.drop(cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_missing = [\"host_acceptance_rate\", \"neighbourhood_group_cleansed\", \"square_feet\", \"weekly_price\" ,\"monthly_price\",\n",
    "                \"license\",\"jurisdiction_names\"]\n",
    "data = data.drop(drop_missing, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location data:\n",
    "\n",
    "neighborhood = [\"street\", \"neighbourhood\",\"neighbourhood_cleansed\"]\n",
    "\n",
    "for i in neighborhood:\n",
    "    print(i, data[i].unique())\n",
    "\n",
    "# These 4 columns above seems to state similar information, therefore, will be deleted, except for 1 representative\n",
    "# --> checking the missing data\n",
    "\n",
    "print(data[neighborhood].isna().sum())# neighbourhood_cleansed will be used\n",
    "\n",
    "# As we work only with data from Prague, columns such as \"city\", \"state\" or market can be dropped, they are same for all,\n",
    "# We add also the three column from above\n",
    "\n",
    "to_drop_location = ['zipcode', 'city', 'state', 'market', 'smart_location',\n",
    "                    'country_code', 'country', 'is_location_exact',\"street\", \"neighbourhood\"]\n",
    "\n",
    "data = data.drop(to_drop_location, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check latitude, longitude, as it is crucial for identifying new features\n",
    "data.latitude.isna().sum()\n",
    "data.longitude.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Columns about number of nights:\n",
    "# These columns seem correlated, so we will test that first:\n",
    "min_max_night = data.loc[:, ['minimum_nights', 'maximum_nights', 'minimum_minimum_nights',\n",
    "       'maximum_minimum_nights', 'minimum_maximum_nights',\n",
    "       'maximum_maximum_nights', 'minimum_nights_avg_ntm',\n",
    "       'maximum_nights_avg_ntm']]\n",
    "\n",
    "corr = min_max_night.corr()\n",
    "sns.heatmap(corr, \n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Therefore, all columns but \"minimum night\" and \"maximum night\" will be dropped\n",
    "data = data.drop(['minimum_minimum_nights', 'maximum_minimum_nights', 'minimum_maximum_nights', \n",
    "                 'maximum_maximum_nights', 'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of listing that host has,\n",
    "# It seems that \"calculated_host_listings_count\" is sum of \n",
    "# \"calculated_host_listings_count_entire_homes\", \"calculated_host_listings_count_private_rooms\", \"calculated_host_listings_count_shared_rooms\"\n",
    "\n",
    "(data.calculated_host_listings_count == (data.calculated_host_listings_count_entire_homes +\n",
    "                                         data.calculated_host_listings_count_private_rooms + \n",
    "                                         data.calculated_host_listings_count_shared_rooms)).unique()\n",
    "\n",
    "# Therefore, three column on the right side will be dropped\n",
    "\n",
    "# host_total_listings_count and calculated_host_listings_count seems to be similar column, threfore\n",
    "# only calculated_host_listings_count will be kept\n",
    "data = data.drop(['host_total_listings_count',\"host_listings_count\", 'calculated_host_listings_count_entire_homes', \n",
    "                  'calculated_host_listings_count_private_rooms', 'calculated_host_listings_count_shared_rooms'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.calculated_host_listings_count.hist() #it seems that more thnan 10k hosts has only 1 listings, which means\n",
    "# It doesnt this column does not add much new information to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# All true/false column will be replaced by 0 and 1\n",
    "data = data.replace({'f': 0, 't': 1})\n",
    "\n",
    "# We plot the distribution of numerical and boolean categories\n",
    "data.hist(figsize=(20,20));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From graphs, we can see that some categorical/boolean columns consists only of one category,therefore will be dropped\n",
    "# Also, columns with obvious majority of one value will be dropped too\n",
    "data = data.drop([\"host_has_profile_pic\", \"has_availability\", \"requires_license\", \"is_business_travel_ready\",\n",
    "                 \"require_guest_profile_picture\", \"require_guest_phone_verification\"], axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable description "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "id - the unique id for each listings<br>\n",
    "experiences_offered - slightly unclear as it does not appear to directly relate to Airbnb Experiences, but this seems to be the main recommended category of travel type, e.g. business <br>\n",
    "host_since - date that the host first joined Airbnb<br>\n",
    "host_response_time - average amount of time the host takes to reply to messages<br>\n",
    "host_response_rate - proportion of messages that the host replies to<br>\n",
    "host_is_superhost - whether or not the host is a superhost, which is a mark of quality for the top-rated and most experienced hosts, and can increase your search ranking on Airbnb<br>\n",
    "calculated_host_listings_count - how many listings the host has in total<br>\n",
    "host_identity_verified - whether or not the host has been verified with id<br>\n",
    "host_neighborhood - Prague districts, where the host is living <br>\n",
    "neighbourhood_cleansed - Prague districts the property is in<br>\n",
    "host_location - district, where host is living <br>\n",
    "property_type - type of property, e.g. house or flat<br>\n",
    "room_type - type of listing, e.g. entire home, private room or shared room<br>\n",
    "accommodates - how many people the property accommodates<br>\n",
    "bathrooms - number of bathrooms<br>\n",
    "bedrooms - number of bedrooms<br>\n",
    "beds - number of beds<br>\n",
    "bed_type - type of bed, e.g. real bed or sofa-bed<br>\n",
    "amenities - list of amenities<br>\n",
    "price - nightly advertised price (the target variable)<br>\n",
    "security_deposit - the amount required as a security deposit<br>\n",
    "cleaning_fee - the amount of the cleaning fee (a fixed amount paid per booking)<br>\n",
    "guests_included - the number of guests included in the booking fee<br>\n",
    "extra_people - the price per additional guest above the guests_included price<br>\n",
    "minimum_nights - the minimum length of stay<br>\n",
    "maximum_nights - the maximum length of stay<br>\n",
    "calendar_updated - when the host last updated the calendar<br>\n",
    "availability_30 - how many nights are available to be booked in the next 30 days<br>\n",
    "availability_60 - how many nights are available to be booked in the next 60 days<br>\n",
    "availability_90 - how many nights are available to be booked in the next 90 days<br>\n",
    "availability_365 - how many nights are available to be booked in the next 365 days<br>\n",
    "number_of_reviews - the number of reviews left for the property<br>\n",
    "number_of_reviews_ltm - the number of reviews left for the property in the last twelve months<br>\n",
    "first_review - the date of the first review<br>\n",
    "last_review - the date of the most recent review<br>\n",
    "review_scores_rating - guests can score properties overall from 1 to 5 stars<br>\n",
    "review_scores_accuracy - guests can score the accuracy of a property's description from 1 to 5 stars<br>\n",
    "review_scores_cleanliness - guests can score a property's cleanliness from 1 to 5 stars<br>\n",
    "review_scores_checkin - guests can score their check-in from 1 to 5 stars<br>\n",
    "review_scores_communication - guests can score a host's communication from 1 to 5 stars<br>\n",
    "review_scores_location - guests can score a property's location from 1 to 5 stars<br>\n",
    "review_scores_value - guests can score a booking's value for money from 1 to 5 stars<br>\n",
    "instant_bookable - whether or not the property can be instant booked (i.e. booked straight away, without having to message the host first and wait to be accepted)<br>\n",
    "cancellation_policy - the type of cancellation policy, e.g. strict or moderate<br>\n",
    "reviews_per_month - calculated field of the average number of reviews left by guest each month<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of individual column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**id** = id is kept as the identification for each variable (for the process of identifying new features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**experiences_offered** = contains only \"none\" value, therefore will be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.experiences_offered.unique()\n",
    "data = data.drop(\"experiences_offered\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**host_since**, **first_review**, **last_review** (*date columns*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These date columns will be convert to \n",
    "# 1) number of days host has been active\n",
    "# 2) number of days the listing is offered\n",
    "\n",
    "# First convert date to date format\n",
    "data[\"june_29\"] = \"2019-06-29\" # date of scraping the data from Airbnb.com\n",
    "data[\"june_29\"]  = pd.to_datetime(data[\"june_29\"], format =\"%Y/%m/%d\")\n",
    "\n",
    "data[\"host_since\"] = data[\"host_since\"].astype('str') \n",
    "data[\"host_since\"] = pd.to_datetime(data[\"host_since\"], format =\"%Y/%m/%d\")\n",
    "\n",
    "data[\"first_review\"] = data[\"first_review\"].astype('str') \n",
    "data[\"first_review\"] = pd.to_datetime(data[\"first_review\"], format =\"%Y/%m/%d\")\n",
    "\n",
    "data[\"last_review\"] = data[\"last_review\"].astype('str') \n",
    "data[\"last_review\"] = pd.to_datetime(data[\"last_review\"], format =\"%Y/%m/%d\")\n",
    "\n",
    "# Derive new columns\n",
    "data[\"days_being_host\"] = (data[\"june_29\"] - data[\"host_since\"]).astype(\"timedelta64[D]\")\n",
    "\n",
    "# I assume listing being active from its first review\n",
    "data[\"days_from_first_review\"] = (data[\"june_29\"] - data[\"first_review\"]).astype(\"timedelta64[D]\")\n",
    "\n",
    "data[\"days_from_last_review\"] = (data[\"june_29\"] - data[\"last_review\"]).astype(\"timedelta64[D]\")\n",
    "\n",
    "# all dates columns\n",
    "date_columns = [\"host_since\", \"first_review\", \"last_review\", \"june_29\"]\n",
    "\n",
    "# drop the original columns that were replaced by age columns\n",
    "data = data.drop(date_columns, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**host_response_time**, **host_response_rate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(data.host_response_time.isna().sum())\n",
    "print(round((data.host_response_time.isna().sum()/len(data))*100, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(data.host_response_time.unique())\n",
    "print()\n",
    "# here \"nan\" will be convert to \"unknown\"\n",
    "data.host_response_time = data.host_response_time.fillna(\"unknown\")\n",
    "print(data.host_response_time.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.host_response_rate.isna().sum())\n",
    "print(round((data.host_response_rate.isna().sum()/len(data))*100, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# as the response rate seems to say similar information as response time (see the numbers below),\n",
    "# we will kept only response_time column\n",
    "print(len(data[data.loc[ :,['host_response_time', 'host_response_rate']].isnull().sum(axis=1) == 2]))\n",
    "data.host_response_rate = data.host_response_rate.fillna(\"unknown\")\n",
    "data.host_response_rate.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(\"host_response_rate\", axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**host_neighbourhood**, **host_location**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.host_neighbourhood.isna().sum())\n",
    "data.host_location.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As host_neighbourhood consists lot of missing data and host_location seems to consist similar/but more general information,\n",
    "# We will use this column (host_location) to create a categorical variable, host_lives_near"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new binary variable based on host_location column\n",
    "data[\"host_location\"] = data[\"host_location\"].astype(str)\n",
    "\n",
    "# initiate the column\n",
    "data[\"host_lives_near\"] = 0\n",
    "# if the column contains Praha or Prague (canse insensitively), assign 1, 0 otherwise\n",
    "data.host_lives_near[data.host_location.str.contains(pat = \"Prague\", case = False)] = 1\n",
    "data.host_lives_near[data.host_location.str.contains(pat = \"Praha\", case = False)] = 1\n",
    "\n",
    "\n",
    "# only 38 is \"unknown\", these category will be considered as 0, (new category for 38 listings will not be created)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.host_lives_near.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column host_location is not needed anymore\n",
    "data = data.drop([\"host_location\", \"host_neighbourhood\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**days_being_host**,**host_is_superhost**, **host_has_profile_pic**, **host_identity_verified**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these 4 columns with characteristics of host has same number of missing values (15)\n",
    "# These rows will be dropped\n",
    "len(data[data.loc[ :,['days_being_host', 'host_is_superhost', 'host_identity_verified'] ].isnull().sum(axis=1) == 3])\n",
    "\n",
    "data = data.dropna(subset=['days_being_host'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**neighbourhood_cleansed**, **longitude**, **latitude**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.neighbourhood_cleansed.unique() # Values will be converted to Prague 1-22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**property_type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.property_type.value_counts()\n",
    "# In this case, we can see a lot of different categories, therefore, we will convert them into\n",
    "# three, \"Apartment\", \"House\" and \"Other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing categories that are types of houses or apartments\n",
    "data.property_type = data.property_type.replace({'Serviced apartment': 'Apartment',\n",
    "                                    'Loft': 'Apartment',\n",
    "                                    \"Condominium\" : \"Apartment\",\n",
    "                                    'Bungalow': 'House',\n",
    "                                    'Cottage': 'House',\n",
    "                                    'Villa': 'House',\n",
    "                                    'Tiny house': 'House',\n",
    "                                    'Earth house': 'House',\n",
    "                                    'Chalet': 'House'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.property_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[~data.property_type.isin(['House', 'Apartment']), 'property_type'] = \"Other\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**room_type**, **accommodates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.room_type.isna().sum())\n",
    "print(data.room_type.unique())\n",
    "# This column seems okay\n",
    "\n",
    "print(data.accommodates.isna().sum())\n",
    "print(data.accommodates.unique())\n",
    "# This column seems okay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**bathrooms**, **bedrooms**, **beds**, **bedtype**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleeping = data.loc[:,[\"bathrooms\", \"bedrooms\", \"beds\", \"bed_type\"]]\n",
    "corr = sleeping.corr()\n",
    "sns.heatmap(corr, \n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values)\n",
    "\n",
    "# number of beds and bedrooms seems correlated\n",
    "\n",
    "# bedrooms vs beds:\n",
    "print(data[\"bedrooms\"].corr(data[\"beds\"])) # 0.6 = quite a lot\n",
    "\n",
    "# check missing values\n",
    "print(sleeping.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# due to correlation, we will leave only \"bedrooms\" variable\n",
    "data = data.drop(\"beds\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(data.bed_type.value_counts()) # bed_type will be dropped, as majority it has bed_type == Real Bed\n",
    "data = data.drop(\"bed_type\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13 values of missing data in bathroom columns will be imputed with median of the number of bathrooms\n",
    "data[\"bathrooms\"]= data[\"bathrooms\"].fillna(data[\"bathrooms\"].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**amenities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of all possible amenities\n",
    "amenities_list = list(data.amenities)\n",
    "amenities_list_string = \" \".join(amenities_list)\n",
    "amenities_list_string = amenities_list_string.replace('{', '')\n",
    "amenities_list_string = amenities_list_string.replace('}', ',')\n",
    "amenities_list_string = amenities_list_string.replace('\"', '')\n",
    "amenities_set = [x.strip() for x in amenities_list_string.split(',')]\n",
    "amenities_set = set(amenities_set)\n",
    "amenities_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the above, some amenities are more important than others,\n",
    "# The amenities chosen to be important (or its combination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24-hour check-in <br>\n",
    "Air conditioning/central air conditioning <br>\n",
    "Amazon Echo/Apple TV/DVD player/game console/Netflix/projector and screen/smart TV (i.e. non-basic electronics) <br>\n",
    "BBQ grill/fire pit/propane barbeque<br>\n",
    "Balcony/patio or balcony<br>\n",
    "Beach view/beachfront/lake access/mountain view/ski-in ski-out/waterfront (i.e. great location/views)<br>\n",
    "Bed linens<br>\n",
    "Breakfast<br>\n",
    "Cable TV/TV<br>\n",
    "Coffee maker/espresso machine<br>\n",
    "Cooking basics<br>\n",
    "Dishwasher/Dryer/Washer/Washer and dryer<br>\n",
    "Elevator<br>\n",
    "Exercise equipment/gym/private gym/shared gym<br>\n",
    "Family/kid friendly, or anything containing 'children'<br>\n",
    "Free parking on premises/free street parking/outdoor parking/paid parking off premises/paid parking on premises<br>\n",
    "Garden or backyard/outdoor seating/sun loungers/terrace<br>\n",
    "Host greets you<br>\n",
    "Hot tub/jetted tub/private hot tub/sauna/shared hot tub/pool/private pool/shared pool<br>\n",
    "Internet/pocket wifi/wifi<br>\n",
    "Long term stays allowed<br>\n",
    "Pets allowed/cat(s)/dog(s)/pets live on this property/other pet(s)<br>\n",
    "Private entrance<br>\n",
    "Safe/security system<br>\n",
    "Self check-in<br>\n",
    "Smoking allowed<br>\n",
    "Step-free access/wheelchair accessible, or anything containing 'accessible'<br>\n",
    "Suitable for events<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['amenities'].str.contains('24-hour check-in'), 'check_in_24h'] = 1\n",
    "data.loc[data['amenities'].str.contains('Air conditioning|Central air conditioning'), 'air_conditioning'] = 1\n",
    "data.loc[data['amenities'].str.contains('Amazon Echo|HBO GO|Game console|Netflix|Projector and screen|Smart TV'), 'high_end_electronics'] = 1\n",
    "data.loc[data['amenities'].str.contains('BBQ grill|Fire pit|Propane barbeque'), 'bbq'] = 1\n",
    "data.loc[data['amenities'].str.contains('Balcony|balcony|Patio|Terrace'), 'balcony'] = 1\n",
    "data.loc[data['amenities'].str.contains('Beach view|Beachfront|Lake access|Mountain view|Ski-in/Ski-out|Waterfront'), 'nature_and_views'] = 1\n",
    "data.loc[data['amenities'].str.contains('Bed linens'), 'bed_linen'] = 1\n",
    "data.loc[data['amenities'].str.contains('Breakfast'), 'breakfast'] = 1\n",
    "data.loc[data['amenities'].str.contains('TV|Cabel TV'), 'tv'] = 1\n",
    "data.loc[data['amenities'].str.contains('Coffee maker|Espresso machine'), 'coffee_machine'] = 1\n",
    "data.loc[data['amenities'].str.contains('Cooking basics'), 'cooking_basics'] = 1\n",
    "data.loc[data['amenities'].str.contains('Dishwasher|Dryer|Washer'), 'white_goods'] = 1\n",
    "data.loc[data['amenities'].str.contains('Elevator'), 'elevator'] = 1\n",
    "data.loc[data['amenities'].str.contains('Exercise equipment|Gym|gym'), 'gym'] = 1\n",
    "data.loc[data['amenities'].str.contains('Family/kid friendly|Children|children'), 'child_friendly'] = 1\n",
    "data.loc[data['amenities'].str.contains('parking'), 'parking'] = 1\n",
    "data.loc[data['amenities'].str.contains('Garden|Outdoor|Sun loungers|Terrace'), 'outdoor_space'] = 1\n",
    "data.loc[data['amenities'].str.contains('Host greets you'), 'host_greeting'] = 1\n",
    "data.loc[data['amenities'].str.contains('Bathtub|Hot tub|Jetted tub|hot tub|Sauna|Pool|pool'), 'hot_tub_sauna_or_pool'] = 1\n",
    "data.loc[data['amenities'].str.contains('Internet|Pocket wifi|Wifi'), 'internet'] = 1\n",
    "data.loc[data['amenities'].str.contains('Long term stays allowed'), 'long_term_stays'] = 1\n",
    "data.loc[data['amenities'].str.contains('Pets|pet|Cat(s)|Dog(s)'), 'pets_allowed'] = 1\n",
    "data.loc[data['amenities'].str.contains('Private entrance'), 'private_entrance'] = 1\n",
    "data.loc[data['amenities'].str.contains('Safe|Security system'), 'secure'] = 1\n",
    "data.loc[data['amenities'].str.contains('Self check-in'), 'self_check_in'] = 1\n",
    "data.loc[data['amenities'].str.contains('Smoking allowed'), 'smoking_allowed'] = 1\n",
    "data.loc[data['amenities'].str.contains('Step-free access|Wheelchair|Accessible'), 'accessible'] = 1\n",
    "data.loc[data['amenities'].str.contains('Suitable for events'), 'event_suitable'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing nulls with zeros for new columns\n",
    "cols_to_replace_nulls = data.loc[:,'check_in_24h':].columns\n",
    "data[cols_to_replace_nulls] = data[cols_to_replace_nulls].fillna(0)\n",
    "\n",
    "# Produces a list of amenity features where one category (true or false) contains fewer than 10% of listings\n",
    "infrequent_amenities = []\n",
    "for col in data.loc[:,'check_in_24h':].columns:\n",
    "    if data[col].sum() < len(data)/10:\n",
    "        infrequent_amenities.append(col)\n",
    "print(infrequent_amenities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping infrequent amenity features\n",
    "data = data.drop(infrequent_amenities, axis=1)\n",
    "\n",
    "# Dropping the original amenity feature\n",
    "data = data.drop('amenities', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**price**, **cleaning_fee**, **security_deposit**, **extra_people**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the data types - numbers:\n",
    "prices =[\"price\", \"cleaning_fee\", \"security_deposit\", \"extra_people\"]\n",
    "\n",
    "for column in prices:\n",
    "    data[column] = data[column].replace({'\\$':''}, regex = True).replace({',':''}, regex = True)\n",
    "    data[column] = data[column].astype(float)\n",
    "    data[column] = data[column].fillna(0)\n",
    "    # if there is no cleaning fee/security deposit/ payment for extra people\n",
    "    # our assumption is: If there is NaN value at the above features\n",
    "    # it means nothing is paid for that ==> 0.\n",
    "    \n",
    "data[prices].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data[prices].isna().sum() # for cleaning_fee and security_deposit, missing values means, that there is no cleaning fee or \n",
    "# security deposit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = data[prices].corr()\n",
    "sns.heatmap(data[prices].corr(), \n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values)\n",
    "# As we can see, security deposit and cleaning fee is almost not correlated with the dependent variable - price, therefore\n",
    "# both will be dropped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**guests_included**, **minimum_nights**, **maximum_nights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.guests_included.isna().sum())\n",
    "print(data.minimum_nights.isna().sum())\n",
    "print(data.maximum_nights.isna().sum())\n",
    "# All three columns semms okay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**calendar updated**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.calendar_updated.value_counts()\n",
    "# Hosts can update their calendar whenever they want\n",
    "# For example when they \"book\" some time, just because they are not able to provide the accommodation, etc.\n",
    "# Therefor, this variable will be dropped, as it is not adding any information to the model\n",
    "data = data.drop(\"calendar_updated\", axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**availability**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Several types of availability occurs in the dataset, obviously correlated with each other\n",
    "availabilty = ['availability_30', 'availability_60', 'availability_90','availability_365']\n",
    "avail = data[availabilty]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As there is no regulation of Airbnb, we will work with the availability_365\n",
    "data = data.drop(['availability_30', 'availability_60', 'availability_90'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**number_of_reviews**, **number_of_reviews_ltm**, **review_per_month**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_reviews = data[[\"number_of_reviews\", \"number_of_reviews_ltm\", \"reviews_per_month\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number_of_reviews is correlated with reviews_per_month and also number of reviews_ltm, \n",
    "# therefore, both of the column will be dropped\n",
    "print(data.number_of_reviews.corr(data.reviews_per_month))\n",
    "print(data.number_of_reviews.corr(data.number_of_reviews_ltm))\n",
    "data = data.drop([\"number_of_reviews_ltm\", \"reviews_per_month\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**review columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review columns are very important for \n",
    "review_cols = ['review_scores_rating',\n",
    "       'review_scores_accuracy', 'review_scores_cleanliness',\n",
    "       'review_scores_checkin', 'review_scores_communication',\n",
    "       'review_scores_location', 'review_scores_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These columns has high number of missing values, however, we would loss lot of information when dropping such column\n",
    "# Reviews are one of the most important aspect when choosing a listing for stay\n",
    "data[review_cols].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_column(col, bins, labels):\n",
    "    \"\"\"\n",
    "    Takes in a column name, bin cut points and labels, replaces the original column with a\n",
    "    binned version\n",
    "    \"\"\"\n",
    "    data[col] = pd.cut(data[col], bins=bins, labels=labels, include_lowest=True)\n",
    "    data[col] = data[col].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Checking the distributions of the review ratings columns\n",
    "variables_to_plot = list(data.columns[data.columns.str.startswith(\"review_scores\") == True])\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "for i, var_name in enumerate(variables_to_plot):\n",
    "    ax = fig.add_subplot(3,3,i+1)\n",
    "    data[var_name].hist(bins=10,ax=ax)\n",
    "    ax.set_title(var_name)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of all review columns that are scored out of 10\n",
    "variables_to_plot.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in variables_to_plot:\n",
    "    bin_column(col,\n",
    "               bins=[0, 8, 9, 10],\n",
    "               labels=['0-8/10', '9/10', '10/10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in variables_to_plot:\n",
    "    data[col] = data[col].replace(\"nan\",\"no reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score 0 to 100\n",
    "bin_column('review_scores_rating',\n",
    "           bins=[0, 80, 95, 100],\n",
    "           labels=['0-79/100', '80-94/100', '95-100/100'])\n",
    "data[\"review_scores_rating\"] = data[\"review_scores_rating\"].replace(\"nan\",\"no reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cancellation policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.cancellation_policy.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace some categories\n",
    "data.cancellation_policy = data.cancellation_policy.replace({\n",
    "            'super_strict_30': 'strict_14_with_grace_period',\n",
    "            'super_strict_60': 'strict_14_with_grace_period',\n",
    "            'strict': 'strict_14_with_grace_period'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**days_from_first_review**, **days_from_last_review**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "periods = ['days_being_host', 'days_from_first_review', 'days_from_last_review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data[periods].isna().sum() # days being host is OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Distribution of the number of days since first review\n",
    "data.days_from_first_review.hist(figsize=(15,5), bins=30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Distribution of the number of days since last review\n",
    "data.days_from_last_review.hist(figsize=(15,5), bins=30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will make from these features categorical variables\n",
    "# Binning time since first review\n",
    "bin_column('days_from_first_review',\n",
    "           bins=[0, 182, 365, 730, 1460, max(data.days_from_first_review)],\n",
    "           labels=['0-6 months',\n",
    "                   '6-12 months',\n",
    "                   '1-2 years',\n",
    "                   '2-3 years',\n",
    "                   '4+ years'])\n",
    "\n",
    "data[\"days_from_first_review\"] = data[\"days_from_first_review\"].replace(\"nan\",\"no reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.days_from_first_review.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning time since last review\n",
    "bin_column('days_from_last_review',\n",
    "           bins=[0, 14, 60, 182, 365, max(data.days_from_last_review)],\n",
    "           labels=['0-2 weeks',\n",
    "                   '2-8 weeks',\n",
    "                   '2-6 months',\n",
    "                   '6-12 months',\n",
    "                   '1+ year'])\n",
    "\n",
    "data[\"days_from_last_review\"] = data[\"days_from_last_review\"].replace(\"nan\",\"no reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.days_from_last_review.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**price**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.price.hist(bins = 100, range = (0,250000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,4))\n",
    "data.price.hist(bins=100, range=(0,100000))\n",
    "plt.margins(x=0)\n",
    "plt.axvline(13500, color='red', linestyle='--')\n",
    "plt.title(\"Airbnb prices in Prague up to 100 000 CZK (~3,780€)\", fontsize=18) # exchange rate on 10/11/2020\n",
    "plt.xlabel(\"Price (CZK)\")\n",
    "plt.ylabel(\"Number of listings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.price.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The price above the 13500 CZK (which is roughly 500 euros) will be considered\n",
    "data = data.loc[data[\"price\"]  <= 13500] # loss of 288 listings\n",
    "data[[\"price\"]].boxplot(figsize = (6,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,4))\n",
    "data.price.hist(bins=100, range=(0,13500))\n",
    "plt.margins(x=0)\n",
    "plt.title(\"Airbnb prices in Prague up to 13,500 CZK (~500€)\", fontsize = 16) # exchange rate on 10/11/2020\n",
    "plt.xlabel(\"Price (CZK)\")\n",
    "plt.ylabel(\"Number of listings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal price considered will be 200 CZK\n",
    "data = data.loc[data[\"price\"]  >= 200] # 1 listing will be dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**calculated_host_listings_count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show top ten hosts with the most listings\n",
    "data.calculated_host_listings_count.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorical and Binary variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prague is divided into 22 city administrative districts, or 57 city districts,\n",
    "# To avoid as much as possible the curse of dimensionality here, we will consider only the administrative districts\n",
    "# Source: https://www.czso.cz/csu/xa/administrativni-a-uzemni-cleneni-prahy\n",
    "data.neighbourhood_cleansed.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['neighbourhood_cleansed'].str.contains('Kunratice'), 'neighbourhood_cleansed'] = \"Praha 4\"\n",
    "\n",
    "data.loc[data['neighbourhood_cleansed'].str.contains('Slivenec'), 'neighbourhood_cleansed'] = \"Praha 5\"\n",
    "\n",
    "data.loc[data['neighbourhood_cleansed'].str.contains('Suchdol'), 'neighbourhood_cleansed'] = \"Praha 6\"\n",
    "data.loc[data['neighbourhood_cleansed'].str.contains('Nebušice'), 'neighbourhood_cleansed'] = \"Praha 6\"\n",
    "data.loc[data['neighbourhood_cleansed'].str.contains('Lysolaje'), 'neighbourhood_cleansed'] = \"Praha 6\"\n",
    "data.loc[data['neighbourhood_cleansed'].str.contains('Přední Kopanina'), 'neighbourhood_cleansed'] = \"Praha 6\"\n",
    "\n",
    "data.loc[data['neighbourhood_cleansed'].str.contains('Troja'), 'neighbourhood_cleansed'] = \"Praha 8\"\n",
    "data.loc[data['neighbourhood_cleansed'].str.contains('Ďáblice'), 'neighbourhood_cleansed'] = \"Praha 8\"\n",
    "data.loc[data['neighbourhood_cleansed'].str.contains('Dolní Chabry'), 'neighbourhood_cleansed'] = \"Praha 8\"\n",
    "data.loc[data['neighbourhood_cleansed'].str.contains('Březiněves'), 'neighbourhood_cleansed'] = \"Praha 8\"\n",
    "\n",
    "data.loc[data['neighbourhood_cleansed'].str.contains('Újezd'), 'neighbourhood_cleansed'] = \"Praha 11\"\n",
    "data.loc[data['neighbourhood_cleansed'].str.contains('Šeberov'), 'neighbourhood_cleansed'] = \"Praha 11\"\n",
    "\n",
    "data.loc[data['neighbourhood_cleansed'].str.contains('Libuš'), 'neighbourhood_cleansed'] = \"Praha 12\"\n",
    "\n",
    "data.loc[data['neighbourhood_cleansed'].str.contains('Řeporyje'), 'neighbourhood_cleansed'] = \"Praha 13\"\n",
    "\n",
    "data.loc[data['neighbourhood_cleansed'].str.contains('Dolní Počernice'), 'neighbourhood_cleansed'] = \"Praha 14\"\n",
    "\n",
    "data.loc[data['neighbourhood_cleansed'].str.contains('Petrovice'), 'neighbourhood_cleansed'] = \"Praha 15\"\n",
    "data.loc[data['neighbourhood_cleansed'].str.contains('Štěrboholy'), 'neighbourhood_cleansed'] = \"Praha 15\"\n",
    "data.loc[data['neighbourhood_cleansed'].str.contains('Dolní Měcholupy'), 'neighbourhood_cleansed'] = \"Praha 15\"\n",
    "data.loc[data['neighbourhood_cleansed'].str.contains('Dubeč'), 'neighbourhood_cleansed'] = \"Praha 15\"\n",
    "\n",
    "\n",
    "data.loc[data['neighbourhood_cleansed'].str.contains('Zbraslav'), 'neighbourhood_cleansed'] = \"Praha 16\"\n",
    "data.loc[data['neighbourhood_cleansed'].str.contains('Velká Chuchle'), 'neighbourhood_cleansed'] = \"Praha 16\"\n",
    "data.loc[data['neighbourhood_cleansed'].str.contains('Lochkov'), 'neighbourhood_cleansed'] = \"Praha 16\"\n",
    "data.loc[data['neighbourhood_cleansed'].str.contains('Lipence'), 'neighbourhood_cleansed'] = \"Praha 16\"\n",
    "\n",
    "data.loc[data['neighbourhood_cleansed'].str.contains('Zličín'), 'neighbourhood_cleansed'] = \"Praha 17\"\n",
    "\n",
    "data.loc[data['neighbourhood_cleansed'].str.contains('Čakovice'), 'neighbourhood_cleansed'] = \"Praha 18\"\n",
    "\n",
    "data.loc[data['neighbourhood_cleansed'].str.contains('Vinoř'), 'neighbourhood_cleansed'] = \"Praha 19\"\n",
    "data.loc[data['neighbourhood_cleansed'].str.contains('Satalice'), 'neighbourhood_cleansed'] = \"Praha 19\"\n",
    "\n",
    "\n",
    "data.loc[data['neighbourhood_cleansed'].str.contains('Klánovice'), 'neighbourhood_cleansed'] = \"Praha 21\"\n",
    "data.loc[data['neighbourhood_cleansed'].str.contains('Koloděje'), 'neighbourhood_cleansed'] = \"Praha 21\"\n",
    "data.loc[data['neighbourhood_cleansed'].str.contains('Běchovice'), 'neighbourhood_cleansed'] = \"Praha 21\"\n",
    "\n",
    "\n",
    "data.loc[data['neighbourhood_cleansed'].str.contains('Benice'), 'neighbourhood_cleansed'] = \"Praha 22\"\n",
    "data.loc[data['neighbourhood_cleansed'].str.contains('Královice'), 'neighbourhood_cleansed'] = \"Praha 22\"\n",
    "data.loc[data['neighbourhood_cleansed'].str.contains('Kolovraty'), 'neighbourhood_cleansed'] = \"Praha 22\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.neighbourhood_cleansed.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neighborhood_toplot = data.groupby('neighbourhood_cleansed').price.median()\n",
    "neighborhood_toplot  = pd.DataFrame(neighborhood_toplot)\n",
    "neighborhood_toplot= neighborhood_toplot.sort_values(\"price\", ascending=False)\n",
    "neighborhood_toplot.plot(kind=\"bar\",figsize=(20,5))\n",
    "plt.title('Median price of Airbnb in Prague districts', fontsize=20)\n",
    "plt.xlabel('Prague districts', fontsize=13)\n",
    "plt.ylabel('Median price (CZK)', fontsize=13)\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data for modeling and further procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_data = pd.get_dummies(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_collinearity_heatmap(df, figsize=(11,9)):\n",
    "    \n",
    "    \"\"\"\n",
    "    Creates a heatmap of correlations between features in the df. A figure size can optionally be set.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set the style of the visualization\n",
    "    sns.set(style=\"ticks\")\n",
    "\n",
    "    # Create a covariance matrix\n",
    "    corr = df.corr()\n",
    "\n",
    "    # Generate a mask the size of our covariance matrix\n",
    "    mask = np.zeros_like(corr, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(corr, mask=mask, cmap=cmap, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, vmax=corr[corr != 1.0].max().max());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "multi_collinearity_heatmap(one_hot_data, figsize=(20,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neighborhood categories seems uncorrelated, only with latitude,longitude, which make sense,\n",
    "# Therefore will be dropped in the future analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "multi_collinearity_heatmap(one_hot_data.drop(list(one_hot_data.columns[one_hot_data.columns.str.startswith('neighbourhood_cleansed')]), axis=1), figsize=(25,22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we tak a look on the first 10 columns (around bathrooms, bedrooms)\n",
    "first_10 = one_hot_data.iloc[:,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "multi_collinearity_heatmap(first_10, figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_data = one_hot_data.drop([\"bedrooms\", \"guests_included\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Take a look again\n",
    "multi_collinearity_heatmap(one_hot_data.drop(list(one_hot_data.columns[one_hot_data.columns.str.startswith('neighbourhood_cleansed')]), axis=1), figsize=(25,22))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Property and room type multi collinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Property and room type multi collinearity\n",
    "room_property = one_hot_data.loc[:, one_hot_data.columns.str.startswith('room_type') | one_hot_data.columns.str.startswith('property_type')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_collinearity_heatmap(room_property, figsize=(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# As we can see, property_type_Other and property_type_Apartment are highly correlated\n",
    "# As well as room_type_Entire home/apt and room_type_Private room\n",
    "# property_type_Apartment and room_type_Private_room (one from each category), otherwise we would have dummy variable trap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.room_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_data = one_hot_data.drop([\"property_type_Apartment\", \"room_type_Private room\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reviews columns multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = one_hot_data.iloc[:,66:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "multi_collinearity_heatmap(reviews, figsize=(20,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apparently, \"noreviews\" column are highly correlated (which is make sense as review information missing almost always\n",
    "# in the same listings), hence these categories will be dropped\n",
    "reviews_to_drop = one_hot_data.columns[one_hot_data.columns.str.endswith('no reviews')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_data = one_hot_data.drop(reviews_to_drop, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_collinearity_heatmap(reviews, figsize=(10,10))\n",
    "# High reviewed listing tends to be in high reviewed columns for each category, therefore these columns are highly correlated\n",
    "# Therefore, we will kept only one category review_score_value as representatives of the others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_score_kept = one_hot_data.loc[:, one_hot_data.columns.str.startswith('review_scores_value')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_scores_to_drop = one_hot_data.columns[one_hot_data.columns.str.startswith('review_scores')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_data = one_hot_data.drop(reviews_scores_to_drop, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_data = pd.concat([one_hot_data,review_score_kept], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_data = one_hot_data.drop(\"host_response_time_unknown\", axis = 1) # correlated with host_response_time_within an hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lastly, we will drop column review_scores_value_9/10, as it is correlated a lot with 10/10\n",
    "one_hot_data[\"review_scores_value_10/10\"].corr(one_hot_data[\"review_scores_value_9/10\"])\n",
    "# Also, we would leave only review_score_10\n",
    "one_hot_data = one_hot_data.drop(\"review_scores_value_9/10\", axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will check the distribution of variables, mainly the dependent variable, price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating X and y\n",
    "X = one_hot_data.drop('price', axis=1)\n",
    "y = one_hot_data.price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=123) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance of the model, `LinearRegression` function from \n",
    "# Scikit-Learn and fit the model on the training data:\n",
    "\n",
    "linreg = LinearRegression()  \n",
    "linreg.fit(X_train, y_train) # training \n",
    "\n",
    "# Now that the model has been fit we can make predictions by calling \n",
    "# the predict command. We are making predictions on the testing set:\n",
    "val_preds_linreg = linreg.predict(X_test)\n",
    "\n",
    "# Check the predictions against the actual values by using the R-2 metrics:\n",
    "print(\"Validation r2:\", round(r2_score(y_test, val_preds_linreg),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_data.to_csv(\"airbnb.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
